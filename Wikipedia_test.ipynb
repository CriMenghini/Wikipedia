{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get in touch with Wikipedia\n",
    "\n",
    "*Remark:* for the tidiness of the `Notebook` the classes and function used are gathered in external libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents\n",
    "1. [Find articles](#parse)\n",
    "2. [Rank articles according to November pageviews](#rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wiki_parser import *\n",
    "from helpers_parser import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Find articles <a name=\"parse\"></a>\n",
    "The fist goal to achive is to find all the Italian and Portuguese Wikipedia articles that mention `Matteo Renzi`. In order to do so,I implemented a [parser](wiki_parser.py) which goes through the raw data and then keeps and stores the `title` and the `text` of the elements of the corpora that mention the [Italian (almost) ex Prime Minister](http://gph.is/29zqwgy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the path of the corpora\n",
    "path = '/Users/cristinamenghini/Downloads/'\n",
    "xml_files = ['itwiki-20161120-pages-articles-multistream.xml', 'ptwiki-20161201-pages-articles-multistream.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a quick peek at a snippet of the `xml`. The elements we are interested in are under the child `page`, which identifies an article. Then we want to get the contents of `title` and `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['      <namespace key=\"2303\" case=\"case-sensitive\">Discussioni definizione accessorio</namespace>\\n',\n",
       " '      <namespace key=\"2600\" case=\"first-letter\">Argomento</namespace>\\n',\n",
       " '    </namespaces>\\n',\n",
       " '  </siteinfo>\\n',\n",
       " '  <page>\\n',\n",
       " '    <title>Armonium</title>\\n',\n",
       " '    <ns>0</ns>\\n',\n",
       " '    <id>2</id>\\n',\n",
       " '    <revision>\\n',\n",
       " '      <id>83431090</id>\\n',\n",
       " '      <parentid>82477294</parentid>\\n',\n",
       " '      <timestamp>2016-09-24T14:25:49Z</timestamp>\\n',\n",
       " '      <contributor>\\n',\n",
       " '        <username>Pufui PcPifpef</username>\\n',\n",
       " '        <id>461628</id>\\n',\n",
       " '      </contributor>\\n',\n",
       " '      <comment>/* Altri progetti */</comment>\\n',\n",
       " '      <model>wikitext</model>\\n',\n",
       " '      <format>text/x-wiki</format>\\n',\n",
       " '      <text xml:space=\"preserve\">{{nota disambigua|il complesso italiano|Armonium (gruppo musicale)}}&lt;!--\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(path + xml_files[0]) as myfile:\n",
    "    head = [next(myfile) for x in range(80)]\n",
    "\n",
    "# Snippet\n",
    "head[-45:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the big size of the `xml` I opted for a parser which registers callbacks for events of interes and then let the parser proceed through the document. The text of the article has not been preprocessed since for the purpose of our analysis we are not going to analyze the text in itself.\n",
    "\n",
    "Hence, I proceed parsing the Italian corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse italian corpus\n",
    "parse_articles('ita', path + xml_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then move towards the Portuguese one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse portuguese corpus\n",
    "parse_articles('port', path + xml_files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The articles are filtered according to the presence of the mention to Matteo Renzi, those in Italian have been stored in a [`.json` file](Corpus/wiki_ita_matteo_renzi.json) whose each line corresponds to a page (`title`, `text`). The same holds for the [articles](Corpus/wiki_port_matteo_renzi.json) in Portuguese. The two corpora are stored in the folder [`Corpus`](https://github.com/CriMenghini/Wikipedia/tree/master/Corpus).\n",
    "\n",
    "                                      {\"title\": \"title_1\", \"text\": \"text_1\"}\n",
    "                                                        ...\n",
    "                                                        ...\n",
    "                                      {\"title\": \"title_n\", \"text\": \"text_n\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank articles according to November pageviews <a name=\"parse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
